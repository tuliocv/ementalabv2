# ===============================================================
# üîó EmentaLabv2 ‚Äî Grafo de Depend√™ncias (v11.5 ‚Äî Chave Global + GPT Fallback Inteligente)
# ===============================================================
# - Usa automaticamente a chave GPT do session_state (sem solicitar novamente)
# - Reinterpreta respostas n√£o estruturadas do GPT (regex A -> B)
# - Fallback autom√°tico SBERT se nada for encontrado
# - Gera gr√°fico hier√°rquico est√°tico com setas direcionadas
# - Gera tabelas organizadas (pr√©-requisitos e dependentes)
# - Produz an√°lise interpretativa (pontos fortes e fracos)
# ===============================================================

from __future__ import annotations
import re
from typing import List, Tuple, Dict
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import networkx as nx

# GPT (opcional)
try:
    from openai import OpenAI
except Exception:
    OpenAI = None

# Utils do projeto
from utils.text_utils import find_col, truncate
from utils.embeddings import sbert_embed, l2_normalize
from utils.exportkit import export_table, show_and_export_fig, export_zip_button


# ---------------------------------------------------------------
# üîç Parsers e heur√≠sticas
# ---------------------------------------------------------------
def _parse_dependencies_with_reasons(text: str) -> List[Tuple[str, str, str]]:
    """Extrai pares A -> B: justificativa"""
    if not text:
        return []
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    triples = []
    pattern = re.compile(r"(.+?)\s*[-‚Äì>]{1,2}\s*(.+?)(?::\s*(.+))?$")
    for ln in lines:
        m = pattern.match(ln)
        if not m:
            continue
        a, b, reason = m.groups()
        a, b = a.strip(" .,:;‚Äì-"), b.strip(" .,:;‚Äì-")
        if a and b and a != b:
            triples.append((a, b, (reason or "‚Äî").strip()))
    seen = set()
    clean = []
    for a, b, r in triples:
        key = (a.lower(), b.lower())
        if key not in seen:
            seen.add(key)
            clean.append((a, b, r))
    return clean


def _infer_semantic_links(df: pd.DataFrame, col_text: str, n_top: int = 2, thr: float = 0.45) -> List[Tuple[str, str, str]]:
    """Fallback SBERT para inferir rela√ß√µes prov√°veis."""
    nomes = df["Nome da UC"].astype(str).tolist()
    textos = df[col_text].astype(str).tolist()
    if len(textos) < 2:
        return []
    emb = l2_normalize(sbert_embed(textos))
    sims = np.dot(emb, emb.T)
    triples = []
    for i, nome_a in enumerate(nomes):
        idx_sorted = np.argsort(-sims[i])
        for j in idx_sorted[1:n_top + 1]:
            if sims[i, j] > thr:
                reason = f"Similaridade sem√¢ntica {sims[i,j]:.2f} entre '{nome_a}' e '{nomes[j]}'"
                triples.append((nome_a, nomes[j], reason))
    triples = list({(a, b, r) for a, b, r in triples if a != b})
    return triples


# ---------------------------------------------------------------
# üìê Layout hier√°rquico est√°vel
# ---------------------------------------------------------------
def _compute_layers(pairs: List[Tuple[str, str, str]]) -> Dict[str, int]:
    """Calcula n√≠veis hier√°rquicos no grafo"""
    G = nx.DiGraph()
    for a, b, _ in pairs:
        G.add_edge(a, b)

    levels = {}
    try:
        order = list(nx.topological_sort(G))
        for node in order:
            preds = list(G.predecessors(node))
            if not preds:
                levels[node] = 0
            else:
                levels[node] = max(levels[p] for p in preds) + 1
        return levels
    except Exception:
        indeg = dict(G.in_degree())
        q = [n for n, d in indeg.items() if d == 0]
        levels = {n: 0 for n in q}
        visited = set(q)
        while q:
            cur = q.pop(0)
            for nxt in G.successors(cur):
                if nxt not in visited:
                    levels[nxt] = max(levels.get(nxt, 0), levels[cur] + 1)
                    q.append(nxt)
                    visited.add(nxt)
        max_layer = max(levels.values()) if levels else 0
        for n in G.nodes:
            if n not in levels:
                levels[n] = max_layer + 1
        return levels


def _layout_by_layers(levels: Dict[str, int], G: nx.DiGraph) -> Dict[str, tuple]:
    """Distribui n√≥s por camadas e espa√ßa horizontalmente"""
    by_layer = {}
    for n, l in levels.items():
        by_layer.setdefault(l, []).append(n)
    for l in by_layer:
        by_layer[l].sort()
    layer_gap_y, node_gap_x = 1.8, 1.4
    pos = {}
    for layer, nodes in by_layer.items():
        width = (len(nodes) - 1) * node_gap_x
        start_x = -width / 2.0
        y = -layer * layer_gap_y
        for i, n in enumerate(nodes):
            pos[n] = (start_x + i * node_gap_x, y)
    return pos


def _draw_static_graph(triples: List[Tuple[str, str, str]], title="Mapa de Depend√™ncias entre UCs"):
    """Desenha grafo hier√°rquico A‚ÜíB"""
    if not triples:
        return None

    G = nx.DiGraph()
    for a, b, _ in triples:
        G.add_edge(a, b)

    levels = _compute_layers(triples)
    pos = _layout_by_layers(levels, G)

    fig, ax = plt.subplots(figsize=(12, 8))
    nx.draw_networkx_nodes(G, pos, node_size=1800, node_color="#E3F2FD", edgecolors="#1976D2", linewidths=1.2, ax=ax)
    nx.draw_networkx_edges(G, pos, arrowstyle="->", arrowsize=18, edge_color="#1976D2", width=2.0, alpha=0.85, ax=ax)
    nx.draw_networkx_labels(G, pos, font_size=9, font_weight="bold", font_color="#1c1c1c", ax=ax)
    ax.set_title(title, fontsize=14, fontweight="bold", pad=16)
    ax.axis("off")
    return fig, levels


# ---------------------------------------------------------------
# üßæ An√°lise interpretativa autom√°tica
# ---------------------------------------------------------------
def _analysis_text(triples: List[Tuple[str, str, str]]) -> str:
    """Resumo interpretativo dos resultados"""
    if not triples:
        return "N√£o foram identificadas rela√ß√µes de pr√©-requisito."

    df_edges = pd.DataFrame(triples, columns=["UC_Base", "UC_Destino", "Justificativa"])
    todas_ucs = sorted(set(df_edges["UC_Base"]).union(set(df_edges["UC_Destino"])))

    deps_por_base = df_edges.groupby("UC_Base")["UC_Destino"].nunique()
    bases_por_dest = df_edges.groupby("UC_Destino")["UC_Base"].nunique()

    uc_influente = deps_por_base.idxmax() if not deps_por_base.empty else None
    uc_dependente = bases_por_dest.idxmax() if not bases_por_dest.empty else None
    isoladas = [uc for uc in todas_ucs if uc not in deps_por_base.index and uc not in bases_por_dest.index]

    n, m = len(todas_ucs), len(df_edges)
    densidade = m / (n * (n - 1)) if n > 1 else 0.0

    parts = []
    parts.append(f"A an√°lise identificou **{m} rela√ß√µes** entre **{n} UCs**, com densidade m√©dia de **{densidade:.2f}**.")
    if uc_influente:
        parts.append(f"**{uc_influente}** √© uma UC **base estruturante**, pois fornece fundamentos para v√°rias outras.")
    if uc_dependente:
        parts.append(f"**{uc_dependente}** √© a UC **mais dependente**, exigindo m√∫ltiplos pr√©-requisitos.")
    if isoladas:
        parts.append(f"Foram encontradas **{len(isoladas)} UCs isoladas** (sem conex√µes diretas): {', '.join(isoladas)}.")
    if densidade > 0.35:
        parts.append("O curr√≠culo demonstra **forte articula√ß√£o vertical e integra√ß√£o entre componentes**.")
    elif densidade > 0.15:
        parts.append("H√° **equil√≠brio** entre autonomia e coer√™ncia curricular entre as UCs.")
    else:
        parts.append("A **baixa densidade** indica poss√≠vel **fragmenta√ß√£o curricular** e oportunidades de refor√ßo conceitual.")
    parts.append("Recomenda-se revisar as UCs isoladas e validar se os encadeamentos est√£o coerentes com o PPC.")
    return "\n\n".join(f"‚úÖ {p}" for p in parts)


# ---------------------------------------------------------------
# üöÄ Fun√ß√£o principal (usa chave global da sess√£o)
# ---------------------------------------------------------------
def run_graph(df: pd.DataFrame, scope_key: str, client=None):
    st.header("üîó Depend√™ncia Curricular")
    st.caption("Identifica rela√ß√µes de **pr√©-requisito (A ‚Üí B)** e interdepend√™ncia entre UCs, "
               "com base nos **Objetos de Conhecimento / Conte√∫dos Program√°ticos**. "
               "Utiliza automaticamente a chave GPT da sess√£o.")

    col_obj = find_col(df, "Objetos de conhecimento") or find_col(df, "Conte√∫do program√°tico")
    if not col_obj:
        st.error("Coluna 'Objetos de conhecimento' (ou 'Conte√∫do program√°tico') n√£o encontrada.")
        return

    base = df[["Nome da UC", col_obj]].dropna().copy()
    if base.empty:
        st.warning("Nenhuma UC com 'Objetos de conhecimento' preenchido.")
        return

    max_uc = st.slider("Quantidade de UCs a considerar (amostra)", 4, min(50, len(base)), min(14, len(base)), 1)
    subset = base.head(max_uc).reset_index(drop=True)

    api_key = st.session_state.get("global_api_key", "")
    triples: List[Tuple[str, str, str]] = []

    # ---------------- GPT ----------------
    if api_key and OpenAI is not None:
        st.success("üß† Modo GPT ativo ‚Äî infer√™ncia sem√¢ntica avan√ßada.")
        try:
            client = OpenAI(api_key=api_key)
            prompt_lines = [
                "TAREFA: identifique **rela√ß√µes diretas de pr√©-requisito (A -> B)** entre as UCs listadas.",
                "FORMATO: A -> B: justificativa curta.",
                "Exemplo: Fundamentos de C√°lculo -> C√°lculo I: base de limites e derivadas.",
                "",
                "UCs (nome: objetos de conhecimento):",
            ]
            for _, r in subset.iterrows():
                prompt_lines.append(f"- {r['Nome da UC']}: {truncate(str(r[col_obj]), 600)}")

            with st.spinner("üß† Analisando depend√™ncias com GPT..."):
                resp = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": "\n".join(prompt_lines)}],
                    temperature=0.0,
                )
            content = (resp.choices[0].message.content or "").strip()
            st.markdown("### üìÑ Sa√≠da do Modelo (para auditoria)")
            st.text_area("Retorno do GPT", value=content, height=220)

            triples = _parse_dependencies_with_reasons(content)

            # üîÑ fallback interno: tenta extrair rela√ß√µes mesmo que o GPT use frases livres
            if not triples:
                pattern = re.findall(r"([A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√áa-z0-9 ,\-()]+)\s*[-‚Äì>]{1,2}\s*([A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√áa-z0-9 ,\-()]+)", content)
                if pattern:
                    triples = [(a.strip(), b.strip(), "Inferido de resposta textual") for a, b in pattern]
                    st.info("‚öôÔ∏è Rela√ß√µes inferidas automaticamente do texto do GPT (formato livre).")
                else:
                    st.warning("‚ö†Ô∏è GPT n√£o retornou pares A -> B em formato v√°lido.")
        except Exception as e:
            st.error(f"‚ùå Erro ao usar GPT: {e}")

    # ---------------- SBERT Fallback ----------------
    if not triples:
        st.warning("üí° Nenhuma liga√ß√£o expl√≠cita via GPT. Aplicando fallback SBERT‚Ä¶")
        triples = _infer_semantic_links(subset, col_obj)

    if not triples:
        st.error("‚ùå Nenhuma rela√ß√£o identificada (nem GPT nem SBERT).")
        export_zip_button(scope_key)
        return

    # ---------------- Gr√°fico ----------------
    st.markdown("### üé® Mapa de Depend√™ncias (A ‚Üí B)")
    fig, levels = _draw_static_graph(triples, "Mapa de Depend√™ncias entre UCs (A ‚Üí B)")
    if fig is not None:
        show_and_export_fig(scope_key, fig, "grafo_dependencias_estatico")
        plt.close(fig)

    # ---------------- Tabelas ----------------
    df_edges = pd.DataFrame(triples, columns=["UC (Pr√©-requisito)", "UC (Dependente)", "Justificativa"])
    st.markdown("### üìò Vis√µes em Tabela")
    tab1, tab2, tab3 = st.tabs(["Pr√©-requisitos da UC", "Esta UC prepara para", "Todas as liga√ß√µes"])

    with tab1:
        req_by_dest = (
            df_edges.groupby("UC (Dependente)")
            .agg({"UC (Pr√©-requisito)": lambda s: ", ".join(sorted(set(s)))})
            .rename(columns={"UC (Pr√©-requisito)": "Pr√©-requisitos"})
            .reset_index()
        )
        st.dataframe(req_by_dest, use_container_width=True)
        export_table(scope_key, req_by_dest, "dependencias_por_destino", "Pr√©-requisitos por UC")

    with tab2:
        deps_by_base = (
            df_edges.groupby("UC (Pr√©-requisito)")
            .agg({"UC (Dependente)": lambda s: ", ".join(sorted(set(s)))})
            .rename(columns={"UC (Dependente)": "UCs que dependem"})
            .reset_index()
        )
        st.dataframe(deps_by_base, use_container_width=True)
        export_table(scope_key, deps_by_base, "dependencias_por_base", "Dependentes por UC")

    with tab3:
        st.dataframe(df_edges, use_container_width=True)
        export_table(scope_key, df_edges, "dependencias_edges", "Rela√ß√µes A ‚Üí B com Justificativa")

    # ---------------- Interpreta√ß√£o ----------------
    st.markdown("### üßæ An√°lise Interpretativa dos Resultados")
    st.markdown(_analysis_text(triples))

    # ---------------- M√©tricas ----------------
    st.markdown("---")
    c1, c2, c3 = st.columns(3)
    c1.metric("UCs analisadas", len(subset))
    c2.metric("Rela√ß√µes identificadas", len(df_edges))
    c3.metric("Camadas (n√≠veis)", len(set(_compute_layers(triples).values())))

    export_zip_button(scope_key)
